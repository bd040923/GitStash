from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, col, explode, expr, lit, regexp_replace, lower
import requests
import json

# Initialize Spark session
spark = SparkSession.builder.appName("OpenDockIngestion").getOrCreate()


# Get the widget values
fullload = dbutils.widgets.get("fullload")
api_url = dbutils.widgets.get("api_url")
auth_url = dbutils.widgets.get("auth_url")
catalogName = dbutils.widgets.get("catalogName")
schemaNamebronze = dbutils.widgets.get("schemaNamebronze")
schemaNamesilver = dbutils.widgets.get("schemaNamesilver")
tableName = dbutils.widgets.get("tableName")
useExternalTables = dbutils.widgets.get("useExternalTables")

if useExternalTables == "Yes":
    #bronze
    container = dbutils.secrets.get(scope="dataplatform-secrets",key="databricks-bronze-storage-container")
    storageAccount = dbutils.secrets.get(scope="dataplatform-secrets",key="databricks-bronze-storage-account")
    adls_base_url = f"abfss://{container}@{storageAccount}.dfs.core.windows.net"
    #silver
    container_silver = dbutils.secrets.get(scope="dataplatform-secrets",key="databricks-silver-storage-container")
    storageAccount_silver = dbutils.secrets.get(scope="dataplatform-secrets",key="databricks-silver-storage-account")
    adls_base_url_silver = f"abfss://{container_silver}@{storageAccount_silver}.dfs.core.windows.net"

else:
    container = dbutils.secrets.get(scope="dataplatform-secrets",key="databricks-storage-container")
    storageAccount = dbutils.secrets.get(scope="dataplatform-secrets",key="databricks-storage-account")


# Function to get JWT token
def get_jwt_token():
    url = auth_url
    payload = {
        "email": dbutils.secrets.get(scope="dataplatform-secrets", key="opendock-user"),
        "password": dbutils.secrets.get(scope="dataplatform-secrets", key="opendock-password")
    }
    
    response = requests.post(url, json=payload)
    if response.status_code == 200:
        return response.json().get("access_token")
    else:
        raise Exception(f"Authentication failed: {response.status_code} - {response.text}")

# Fetch JWT token
jwt_token = get_jwt_token()

# Function to fetch appointments with pagination and lastChangedDateTime parameter for incremental load
def fetch_appointments(last_changed_time=None):
    headers = {
        "Authorization": f"Bearer {jwt_token}",
        "Content-Type": "application/json"
    }
    
    params = {}
    if last_changed_time:
        params["lastChangedDateTime"] = last_changed_time  # Fetch >= provided timestamp
    
    response = requests.get(api_url, headers=headers, params=params)
    
    if response.status_code == 200:
        return response.json().get("data", [])  # Return the "data" part of the JSON response
    else:
        raise Exception(f"Failed to fetch appointments: {response.status_code} - {response.text}")

# Full load function
def full_load():

    if tableName == "opd_appointments":
    
        appointments_data = fetch_appointments()
        if not appointments_data:
            raise ValueError("No data returned from API.")

        # Convert JSON response to Spark DataFrame
        rdd = spark.sparkContext.parallelize(appointments_data)
        df = spark.read.json(rdd.map(lambda x: json.dumps(x)))

    else:
        # Fetch all appointments for the full load
        appointments_data = fetch_appointments()

        # Convert the fetched data to JSON string and load it into a Spark DataFrame
        rdd = spark.sparkContext.parallelize([json.dumps(appointments_data)])
        
        # Read the JSON as a DataFrame to infer schema automatically
        df = spark.read.json(rdd)

    # Create Bronze table with extracted data and schema
    if useExternalTables == "Yes":
            location_path_bronze = f"{adls_base_url}/{tableName}"
            df.withColumn("load_timestamp", current_timestamp()) \
            .write.mode("overwrite").option("overwriteSchema", "true").option("path", location_path_bronze).format("delta").saveAsTable(f"{catalogName}.{schemaNamebronze}.{tableName}")
    else:
            df.withColumn("load_timestamp", current_timestamp()) \
            .write.mode("overwrite").option("overwriteSchema", "true").format("delta").saveAsTable(f"{catalogName}.{schemaNamebronze}.{tableName}")
    
    print(f"Bronze table {catalogName}.{schemaNamebronze}.{tableName} created successfully.")
    create_silver_table(full_load=True)  # Call to create Silver table after full load

# Delta (incremental) load function with upsert logic
def delta_load():
    # Get the latest timestamp from the Bronze table
    last_changed_time = spark.sql(f"SELECT MAX(lastChangedDateTime) FROM {catalogName}.{schemaNamebronze}.{tableName}").collect()[0][0]
    

    if tableName == "opd_appointments":
    
        # appointments_data = fetch_appointments()
        appointments_data = fetch_appointments(last_changed_time)

        if not appointments_data:
            raise ValueError("No data returned from API.")

        # Convert JSON response to Spark DataFrame
        rdd = spark.sparkContext.parallelize(appointments_data)
        df = spark.read.json(rdd.map(lambda x: json.dumps(x)))

    else:
        
        # Fetch only the updated or new appointments since the last load
        appointments_data = fetch_appointments(last_changed_time)


        # Convert the fetched data to JSON string and load it into a Spark DataFrame
        rdd = spark.sparkContext.parallelize([json.dumps(appointments_data)])
        
        # Read the JSON as a DataFrame to infer schema automatically
        df = spark.read.json(rdd)

    # Add the current load timestamp
    df = df.withColumn("load_timestamp", current_timestamp())

    # Create a temporary view from the fetched data
    df.createOrReplaceTempView("updates")

    # Upsert logic using MERGE INTO to update existing records and insert new records
    merge_query = f"""
    MERGE INTO {catalogName}.{schemaNamebronze}.{tableName} AS target
    USING updates AS source
    ON target.id = source.id
    WHEN MATCHED THEN
      UPDATE SET *
    WHEN NOT MATCHED THEN
      INSERT *
    """
    
    # Execute the merge query to perform the upsert
    spark.sql(merge_query)
    
    print(f"Upsert completed for Bronze table {catalogName}.{schemaNamebronze}.{tableName}.")
    create_silver_table(full_load=False)  # Call to create/update Silver table after delta load

# Function to create or update the Silver table from the Bronze table
def create_silver_table(full_load):

    if tableName == "opd_appointments":
            # Load the bronze DataFrame
            bronze_df = spark.table(f"{catalogName}.{schemaNamebronze}.{tableName}")

            # Step 2: Flatten the `customFields` array into separate columns
            custom_fields_df = bronze_df.withColumn("customField", explode("customFields"))

            # Extract the `label` and `value` from the `customField` column
            custom_fields_df = custom_fields_df.select(
                "*",
                col("customField.label").alias("customFieldLabel"),
                col("customField.value").alias("customFieldValue")
            )

            # Clean and standardize the customFieldLabel by replacing invalid characters and ensuring lowercase
            custom_fields_df = custom_fields_df.withColumn(
                "customFieldLabelCleaned",
                lower(regexp_replace(col("customFieldLabel"), r'[^a-zA-Z0-9]', '_'))  # Replace special characters and spaces
            )

            # Pivot the custom fields to create individual columns dynamically
            custom_fields_pivot_df = custom_fields_df.groupBy("id").pivot("customFieldLabelCleaned").agg(expr("first(customFieldValue)"))

            # Step 3: Flatten the `statusTimeline` column
            status_timeline_df = bronze_df.select(
                "*",
                col("statusTimeline.Arrived").alias("status_Arrived"),
                col("statusTimeline.Cancelled").alias("status_Cancelled"),
                col("statusTimeline.Completed").alias("status_Completed"),
                col("statusTimeline.InProgress").alias("status_InProgress"),
                col("statusTimeline.NoShow").alias("status_NoShow"),
                col("statusTimeline.Requested").alias("status_Requested"),
                col("statusTimeline.Scheduled").alias("status_Scheduled")
            )

            # Step 4: Join the flattened custom fields and statusTimeline back to the main dataframe
            final_df = status_timeline_df.alias("status").join(
                custom_fields_pivot_df.alias("customFields"),
                on="id",
                how="left"
            )

            # Step 5: Select all columns except 'customFields' and 'statusTimeline'
            excluded_columns = ['customFields', 'statusTimeline']
            final_columns = [col for col in final_df.columns if col not in excluded_columns]

            # Apply the selection
            final_df = final_df.select(*final_columns)
    
    else:
            # Step 1: Read data from bronze table
            final_df = spark.table(f"{catalogName}.{schemaNamebronze}.{tableName}")

    # Step 6: Write the final dataframe to the Silver table
    if full_load:
            if useExternalTables == "Yes":
                location_path_silver = f"{adls_base_url_silver}/{tableName}"
                final_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").option("path", location_path_silver).saveAsTable(f"{catalogName}.{schemaNamesilver}.{tableName}")

                print(f"Silver table {catalogName}.{schemaNamesilver}.{tableName} created successfully.")
            else:
                final_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(f"{catalogName}.{schemaNamesilver}.{tableName}")
                print(f"Silver table {catalogName}.{schemaNamesilver}.{tableName} created successfully.")
    else:
            # For delta load, perform an upsert into the Silver table
            final_df.createOrReplaceTempView("silver_updates")
            
            merge_query = f"""
            MERGE INTO {catalogName}.{schemaNamesilver}.{tableName} AS target
            USING silver_updates AS source
            ON target.id = source.id
            WHEN MATCHED THEN
            UPDATE SET *
            WHEN NOT MATCHED THEN
            INSERT *
            """
            
            spark.sql(merge_query)
            print(f"Silver table {catalogName}.{schemaNamesilver}.{tableName} updated successfully.")

# Determine which load to run based on the widget selection
if fullload == "True":
    print("Running full load...")
    full_load()
else:
    print("Running delta load with upsert...")
    delta_load()
